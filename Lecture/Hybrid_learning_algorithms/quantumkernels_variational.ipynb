{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Creating a variational classifier with PennyLane"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the guest lecture on quantum kernels, we saw in Section 4 that feature embeddings can be used in combination with variational classifiers that analyse data in \"quantum feature space\". This notebook shows a simplified implementation of the idea in the PennyLane framework for hybrid optimization. \n",
                "\n",
                "Since training variational quantum circuits can be time consuming, we show what decision boundaries for a simple classification task a quantum classifier gives rise to.\n",
                "\n",
                "### The variational circuit\n",
                "\n",
                "The circuit we discussed contains the following elements:\n",
                "1. a quantum circuit $S_x$ that depends on an input $x$, which maps the input to a quantum state,\n",
                "2. a variational circuit $U_{\\theta}$, which depends on parameters $\\theta$ that can be optimized,\n",
                "3. a computational basis measurement of the first qubit. \n",
                "\n",
                "The routine can be interpreted as a classifier that takes inputs $x$ and maps them to outputs $y = \\langle \\hat{\\sigma}_z\\rangle$. The classifier is \"trained\" by finding parameters $\\theta$ given data of input-output samples."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<img src=\"figures\/variational_classifier.png\" alt=\"circuit\" style=\"width:400px;\"\/>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For this tutorial to run, you need to install the PennyLane library. In most cases this can be done with \n",
                "\n",
                "    pip install pennylane \n",
                "\n",
                "**An extended example and more details can be found at *pennylane.ai*.**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Importing libraries"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let us import some helpers from scikit-learn:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "from sklearn.datasets import make_classification\n",
                "from sklearn.preprocessing import scale\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If you installed the PennyLane library correctly, you can run the following imports."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import pennylane as qml\n",
                "from pennylane import numpy as np\n",
                "from pennylane.optimize import GradientDescentOptimizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We also need matplotlib for visualisation. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline  "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Implementing the circuit"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As a quantum device we use PennyLane's default qubit simulator with 2 qubits. We can create the quantum device as follows. \n",
                "\n",
                "*Note 1: using another backend than `default.qubit`, we can also run the circuit on a real quantum device through a cloud service.*\n",
                "\n",
                "*Note 2: the keyword `wires` stands for the number of qubits. Since PennyLane can also handle non-qubit (i.e., continuous-variable systems), wires generally refers to the number of subsystems.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "dev = qml.device('default.qubit', wires=2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now implement the two quantum circuits in PennyLane, using rather arbitrary gate sequences (find better ones!). Let is first define a feature embedding (or state preparation) circuit $S_x$..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def S(x): \n",
                "    qml.RX(x[0], 0)\n",
                "    qml.RX(x[1], 1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "... and a classification circuit $U(\\theta)$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def U(theta):  \n",
                "    qml.Rot(theta[0], theta[1], theta[2], 0)\n",
                "    qml.Rot(theta[3], theta[4], theta[5], 0)\n",
                "    qml.CNOT([1, 0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We call these two routines in a quantum classifier function. In fact, $U(\\theta)$ is called several times, depending on how many lists of parameters `thetas` contains. After this, the expectation of the Pauli Z operator is measured with respect to the final state. \n",
                "\n",
                "The line \"@qml.qnode(dev)\" is a *decorator* that turns the function into a QNode object, PennyLane's representation of quantum computations. Note that in `qclassifier` no classical processing (e.g., addition, multiplication, etc) is allowed, but only quantum information processing (gates and expectation values). You can find more information on https:\/\/pennylane.readthedocs.io\/en\/latest\/code\/qnode.html ."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "num_layers = 3\n",
                "\n",
                "@qml.qnode(dev)\n",
                "def qclassifier(thetas, x=None):\n",
                "    S(x)\n",
                "    for theta in thetas:\n",
                "        U(theta)\n",
                "    return qml.expval.PauliZ(0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For optimization we need a cost function. The cost compares targets Y and model outputs with a square loss function. PennyLane needs to be able to \"pass gradients\" through these functions, which is ensured by previously importing PennyLane's native numpy library version as `np`. (This import makes most numpy operations accessible to automatic differentiation; in other words, PennyLane can compute gradients of functions constructed from QNodes and numpy operations)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def cost(theta, X, Y):\n",
                "    Y_ = np.array([qclassifier(theta, x=x) for x in X])\n",
                "    loss = np.mean(np.abs(Y - Y_)**2)\n",
                "    return loss "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Loading data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We generate a simple dataset of two features. To simplify things we will not split it into training and validation sets but just try to fit the data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "X, y = make_classification(n_samples=30, n_features=2, n_informative=2, n_redundant=0, \n",
                "                           n_repeated=0, n_classes=2, random_state=6, class_sep=1.4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can visualise the data with matplotlib."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure()\n",
                "plt.scatter(X[:, 0][y==0], X[:, 1][y==0], c='r', marker='^', edgecolors='k', label=\"train 0\")\n",
                "plt.scatter(X[:, 0][y==1], X[:, 1][y==1], c='b', marker='^', edgecolors='k', label=\"train 1\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualising the decision boundary"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's have a look at the decision boundaries that the quantum classifier \"naturally\" gives rise to if it is not trained.\n",
                "\n",
                "You can change the parameter `num_layers` above and run its cell again to play around with the shape of the decision boundary. (The parameter is defined in the same cell as the `qclassifier` in order to make sure you redefine the classifier as well once `num_layers` changes.)\n",
                "\n",
                "*PS: The plot takes some seconds to be built.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "np.random.seed(10)\n",
                "thetas = 3*np.random.rand(num_layers, 6)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# make data grid for contour plot\n",
                "xx, yy = np.meshgrid(np.linspace(-2.5, 2.5, 20), np.linspace(-2.5, 2.5, 20))\n",
                "X_grid = [np.array([x, y]) for x, y in zip(xx.flatten(), yy.flatten())]\n",
                "\n",
                "# plot decision regions\n",
                "plt.figure()\n",
                "cm = plt.cm.RdBu\n",
                "predictions_grid = [qclassifier(thetas, x=x) for x in X_grid]\n",
                "Z = np.reshape(predictions_grid, xx.shape)\n",
                "cnt = plt.contourf(xx, yy, Z, levels=np.arange(-1, 1.1, 0.2), cmap=cm, alpha=.8)\n",
                "plt.colorbar(cnt, ticks=[0, 0.5, 1])\n",
                "\n",
                "# plot data\n",
                "plt.scatter(X[:, 0][y==0], X[:, 1][y==0], c='r', marker='^', edgecolors='k', label=\"train 0\")\n",
                "plt.scatter(X[:, 0][y==1], X[:, 1][y==1], c='b', marker='^', edgecolors='k', label=\"train 1\")\n",
                "\n",
                "plt.ylim(-2.5, 2.5)\n",
                "plt.xlim(-2.5, 2.5)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The model can be trained by updating the parameters with stochastic gradient descent. In each step, a fair bit of quantum computation is simulated in the background to get gradients for each data point, and training takes some time. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "o = GradientDescentOptimizer(0.01)\n",
                "\n",
                "for it in range(40):\n",
                "    \n",
                "    thetas = o.step(lambda v: cost(v, X, y), thetas)\n",
                "    print(\"Cost in step\" , it+1 , \":\", cost(thetas, X, y))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, we can evaulate the accuracy on the training set, comparing model predictions with the target labels `y`. If the accuracy is `1.0`, all data points have been classified correctly by the model. Of course, this does not tell us anything about the model's generalisation performance.\n",
                "\n",
                "*You can run the cell above again to train for another 40 steps.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pred = [1 if qclassifier(thetas, x=x_) > 0 else 0 for x_ in X]\n",
                "accuracy_score(pred, y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's plot the new decision regions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# make data for plot\n",
                "xx, yy = np.meshgrid(np.linspace(-2.5, 2.5, 20), np.linspace(-2.5, 2.5, 20))\n",
                "X_grid = [np.array([x, y]) for x, y in zip(xx.flatten(), yy.flatten())]\n",
                "\n",
                "# start plot\n",
                "plt.figure()\n",
                "cm = plt.cm.RdBu\n",
                "\n",
                "# plot decision regions\n",
                "predictions_grid = [qclassifier(thetas, x=x) for x in X_grid]\n",
                "Z = np.reshape(predictions_grid, xx.shape)\n",
                "cnt = plt.contourf(xx, yy, Z, levels=np.arange(-1, 1.1, 0.2), cmap=cm, alpha=.8)\n",
                "plt.colorbar(cnt, ticks=[0, 0.5, 1])\n",
                "\n",
                "# plot data\n",
                "plt.scatter(X[:, 0][y==0], X[:, 1][y==0], c='r', marker='^', edgecolors='k', label=\"train 0\")\n",
                "plt.scatter(X[:, 0][y==1], X[:, 1][y==1], c='b', marker='^', edgecolors='k', label=\"train 1\")\n",
                "\n",
                "plt.ylim(-2.5, 2.5)\n",
                "plt.xlim(-2.5, 2.5)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 [3.6]",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}